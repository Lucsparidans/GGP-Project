LOG 1641837509956 [L3]: 
Logging settings for gamer SAPPAMctsGamer.
SETTINGS_FILE = C:/Users/lucsp/git/GGP-Project/GamersSettings/SAPPA.properties
STATE_MACHINE_TYPE = CompactStateMachine(SeparateInternalPropnetStateMachine)
PROPNET_MACHINE_TYPE = SeparateInternalPropnetStateMachine
USE_PROVER = false
PROPNET_BUILD = NEVER
BUILD_PN_SAFETY_MARGIN = 5000
PROVER_CACHE = true
PROPNET_CACHE = false
METAGAME_SAFETY_MARGIN = 10000
SELECT_MOVE_SAFETY_MARGIN = 10000
METAGAME_SEARCH = true

MCTS_MANAGER_TYPE = HybridMctsManager

FLIP_PROBABILITY = 0.0
FLIP_WINS_ONLY = true
MAX_SEARCH_DEPTH = 500
NUM_EXPECTED_ITERATIONS = -1
TREE_NODE_FACTORY = AmafDecoupledTreeNodeFactory
SELECTION_STRATEGY = PlayoutSupportedSelection
  T = 
    NAME = T
    FIXED_VALUE = 0.0
    TUNING_ORDER_INDEX = -1
    current_values = [ 0.0 0.0 ]
  SUB_SELECTION = GraveSelection
    VALUE_OFFSET = 
      NAME = VO
      FIXED_VALUE = 0.01
      TUNING_ORDER_INDEX = -1
      current_values = [ 0.01 0.01 ]
    MOVE_EVALUATOR = GraveEvaluator
      C = 
        NAME = C
        FIXED_VALUE = 0.2
        TUNING_ORDER_INDEX = -1
        current_values = [ 0.2 0.2 ]
        POSSIBLE_VALUES = [ 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ]
        POSSIBLE_VALUES_PENALTY = null
      FPU = 
        NAME = Fpu
        FIXED_VALUE = 1.0
        TUNING_ORDER_INDEX = -1
        current_values = [ 1.0 1.0 ]
      BETA_COMPUTER = CadiaBetaComputer
        K = 
          NAME = K
          FIXED_VALUE = 250.0
          TUNING_ORDER_INDEX = -1
          current_values = [ 250.0 250.0 ]
          POSSIBLE_VALUES = [ 0.0 10.0 50.0 100.0 250.0 500.0 750.0 1000.0 2000.0 inf ]
          POSSIBLE_VALUES_PENALTY = null
      DEFAULT_EXPLORATION = 1.0
      closest_amaf_stats = [ null null ]
    MIN_AMAF_VSITS = 
      NAME = Ref
      FIXED_VALUE = 50.0
      TUNING_ORDER_INDEX = -1
      current_values = [ 50.0 50.0 ]
      POSSIBLE_VALUES = [ -1.0 0.0 50.0 100.0 250.0 500.0 1000.0 10000.0 inf ]
      POSSIBLE_VALUES_PENALTY = null
  SUB_PLAYOUT = MovesAndSiblingsMemorizingStandardPlayout
EXPANSION_STRATEGY = NoExpansion
PLAYOUT_STRATEGY = MovesAndSiblingsMemorizingStandardPlayout
  CURRENT_SIMULATION_JOINT_MOVES = 26 entries.
  JOINT_MOVE_SELECTOR = GibbsAdaptivePlayoutMoveSelector
    ppa_weights = [ 0 entries, 0 entries, ]
    PPA_TEMPERATURE = 1.0
BACKPROPAGATION_STRATEGY = GraveAdaptivePlayoutBackpropagation
  NODE_UPDATER_1 = StandardUpdater
  NODE_UPDATER_2 = GraveUpdater
MOVE_CHOICE_STRATEGY = MaximumScoreChoice
BEFORE_SEARCH_STRATEGY = null
AFTER_METAGAME_STRATEGY = null
BEFORE_SIM_STRATEGY = TunerBeforeSimulation
  SIM_BUDGET = 2147483647
  BATCH_SIZE = 1
  PARAMETERS_TUNER = StandardMultiPopEvoParametersTuner
    DISCRETE_PARAMETERS_MANAGER = DiscreteParametersManager
      DISCRETE_TUNABLE_PARAMETER = [ 
        DISCRETE_TUNABLE_PARAMETER = 
        NAME = K
        FIXED_VALUE = 250.0
        TUNING_ORDER_INDEX = -1
        current_values = [ 250.0 250.0 ]
        POSSIBLE_VALUES = [ 0.0 10.0 50.0 100.0 250.0 500.0 750.0 1000.0 2000.0 inf ]
        POSSIBLE_VALUES_PENALTY = null
        DISCRETE_TUNABLE_PARAMETER = 
        NAME = Alpha
        FIXED_VALUE = 0.1
        TUNING_ORDER_INDEX = -1
        current_values = [ 0.1 0.1 ]
        POSSIBLE_VALUES = [ 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ]
        POSSIBLE_VALUES_PENALTY = null
        DISCRETE_TUNABLE_PARAMETER = 
        NAME = Ref
        FIXED_VALUE = 50.0
        TUNING_ORDER_INDEX = -1
        current_values = [ 50.0 50.0 ]
        POSSIBLE_VALUES = [ -1.0 0.0 50.0 100.0 250.0 500.0 1000.0 10000.0 inf ]
        POSSIBLE_VALUES_PENALTY = null
        DISCRETE_TUNABLE_PARAMETER = 
        NAME = C
        FIXED_VALUE = 0.2
        TUNING_ORDER_INDEX = -1
        current_values = [ 0.2 0.2 ]
        POSSIBLE_VALUES = [ 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ]
        POSSIBLE_VALUES_PENALTY = null
]
      INITIAL_PARAMETERS_ORDER = RandomOrder
      index_of_K = 0
      index_of_Ref = 2
    NUM_COMBINATORIAL_MOVES = 5913
    TUNE_ALL_ROLES = false
    RANDOM-OPPONENTS = true
    REUSE_BEST_COMBOS = false
    REUSE_STATS = false
    tuning = true
    LOG_POPULATIONS = false
    DISCRETE_EVOLUTION_MANAGER = StandardEvolutionManager
      POPULATION_SIZE = 50
      ELITE_SIZE = 25
      PARAMETERS_MANAGER = DiscreteParametersManager
      CROSSOVER_PROBABILITY = 0.5
      CROSSOVER_MANAGER = UniformCrossover
        DISCRETE_PARAMETERS_MANAGER = DiscreteParametersManager
      MUTATION_MANAGER = SingleRandomMutation
        PARAMETERS_MANAGER = DiscreteParametersManager
    BEST_COMBINATION_SELECTOR = UcbSelector
      BIAS_COMPUTER = null
      C = 0.0
      VALUE_OFFSET = 0.0
      FPU = 0.0
    EVALUATE_ALL_COMBOS_OF_INDIVIDUALS = false
    INDIVIDUALS_ITERATOR = RandomCombosOfIndividualsIterator
    EVAL_REPETITIONS = 1
    eval_repetitions_count = -1
    num_role_problems = 1
    selected_combinations_indices = [ [ 0 0 0 0 ] ]
    best_combinations_indices = null
  sim_count = 0
AFTER_SIM_STRATEGY = CompositeAfterSimulation
  AFTER_SIM_STRATEGY_0 = TunerAdaptivePlayoutAfterSimulation
    ALPHA = org.ggp.base.player.gamer.statemachine.MCTS.manager.parameterstuning.structure.parameters.DiscreteTunableParameter@3cbcec5d
    INVERT = false
    ALPHA_DISCOUNT = 1.0
    UPDATE_TYPE = WINS
    ppa_weights = [ 0 entries, 0 entries, ]
  AFTER_SIM_STRATEGY_1 = TunerAfterSimulation
    PARAMETERS_TUNER = StandardMultiPopEvoParametersTuner
  AFTER_SIM_STRATEGY_2 = GraveAfterSimulation
BEFORE_MOVE_STRATEGY = null
AFTER_MOVE_STRATEGY = CompositeAfterMove
  AFTER_MOVE_STRATEGY_0 = AdaptivePlayoutAfterMove
    DECAY_FACTOR = 1.0
    LOG_WEIGHTS = false
    ppa_weights = [ 0 entries, 0 entries, ]
  AFTER_MOVE_STRATEGY_1 = TunerAfterMove
    PARAMETERS_TUNER = StandardMultiPopEvoParametersTuner
    LOG = false
AFTER_GAME_STRATEGY = TunerAfterGame
  PARAMETERS_TUNER = StandardMultiPopEvoParametersTuner
  LOG_AFTER_GAME = false
TRANSPOSITION_TABLE = MctsTranspositionTable
  LOGGING = false
  TREE_DECAY = 1.0
  AMAF_DECAY = 1.0
  GAME_STEP_OFFSET = 2
LOG_LOG_TREE = false
LOG_TREE_BRANCHING_AND_DEPTH_AND_ENTROPY = false
abstract_state_machine = CompactStateMachine(SeparateInternalPropnetStateMachine)
num_roles = 2
my_role_index = 1
current_game_step = 0
step_score_sum_for_role = [ 0.0 0.0 ]
step_iterations = 0
step_visited_nodes = 0.0
step_added_nodes = 0
step_memorized_states = 0
step_search_duration = 0
current_iteration_visited_nodes = 0.0
